{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-22T14:36:43.504307Z",
     "start_time": "2020-09-22T14:32:10.269170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: np_binary_0.0001_8_2009101037_199_t_accr_0.9819_t_loss_0.331068_v_accr_0.9400_v_loss_0.373449.pt \n",
      "\n",
      "model inference, processing, and saving\n",
      "\n",
      "1 : Gang dae gue (30423812) 28 Oct 16_1.mpg 19390 \n",
      "\n",
      "19390 |################################################################################ | 100.0% loading frames\n",
      "19390: 1212(*16) |###############----------------------------------------------------------------- | 19.1% model prediction"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e1f5a8a9d838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Binary Model과 이미지 파일을 model_pred_and_time 함수에 넣고 리턴으로 binary model의 예측 결과와 걸린 시간을 가져옴\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# 예측결과는 model의 softmax 결과 값이 담긴 list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes_pre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# 각각의 예측 모델이 예측한 frame은 softmax의 argmax 결과가 1인 index에 해당하는 frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disk1/yunseob/Pytorch/1_CapsuleEndo/algorithms/ce_utils/video.py\u001b[0m in \u001b[0;36mmodel_pred\u001b[0;34m(model, frames_pre)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes_pre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReshape4torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes_pre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mpred_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disk1/yunseob/Pytorch/1_CapsuleEndo/algorithms/ce_model/CNN_inference.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, x, batch_size)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m#             score, pred = self.Model_pred(self.model, test_X[batch, :, :, :], test_Y[batch])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             printProgress(i+1, len(batches), prefix = '{}: {}(*{})'.format(len(x), len(batches), batch_size), \n",
      "\u001b[0;32m/mnt/disk1/yunseob/Pytorch/1_CapsuleEndo/algorithms/ce_model/CNN_inference.py\u001b[0m in \u001b[0;36mprediction\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m#         return torch.tensor(output, device = 'cpu').numpy(), pred.tolist()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('/mnt/disk1/yunseob/Pytorch/1_CapsuleEndo/algorithms')\n",
    "# from ce_utils.record import printProgress, Sec_to_M_S_ms\n",
    "# from ce_utils.video import load_frame, find_clip_idxs, merging, captioning\n",
    "# from ce_utils.data import batch_idxs, Reshape4torch\n",
    "# from ce_model.CNN_inference import CNN_model\n",
    "from ce_utils.video import load_frames, model_pred, find_clip_idxs, get_gradcam, process_and_save\n",
    "from ce_utils.record import total_sum_time\n",
    "from ce_model.CNN_inference import CNN_model\n",
    "\n",
    "import argparse\n",
    "\n",
    "def parse_arguments():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--model_dir', action=\"store\", type=str, \n",
    "                        default='/0 data/video/200713 clip for AI re-screening', help='model directory')\n",
    "    \n",
    "    parser.add_argument('--model_name', action=\"store\", type=str, \n",
    "                        default='np_binary', help='model name')\n",
    "    \n",
    "    parser.add_argument('--network', action=\"store\", type=str, \n",
    "                        default='CNN_v1', help='network')\n",
    "    \n",
    "    parser.add_argument('--gpu_idx', action=\"store\", type=int, \n",
    "                        default=3, help='gpu idx used for training')\n",
    "    \n",
    "    parser.add_argument('--frame_root', action=\"store\", type=str, \n",
    "                        default='/mnt/disk2/data/private_data/SMhospital/capsule/0 data/video/200917 videos for AI feedback', \n",
    "                        help='frame directory')\n",
    "    \n",
    "    parser.add_argument('--save_root', action=\"store\", type=str, \n",
    "                        default='./np_binary', help='save directory')\n",
    "    \n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    print('args={}'.format(args))\n",
    "    \n",
    "    return args\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    model = CNN_model(network = args.network, n_ch = 3, n_cls = 2, \n",
    "                      model_dir = args.model_dir, model_name = args.model_name, gpu_idx = args.gpu_idx)\n",
    "\n",
    "\n",
    "    # readme.txt 읽어옴\n",
    "    video_log = pd.read_csv(args.frame_root + '/readme.txt', header = 0, delimiter = '|', encoding = \"utf-8\")\n",
    "\n",
    "\n",
    "    # 최종 처리 이후 csv 파일에 들어갈 column 이름들 (for statistical analysis)\n",
    "    record = pd.DataFrame(columns = ['index', 'video', '# of frames', 'video length', 'save frame time', 'load frame time', \n",
    "                                     'pred. time', '# of positive frames', '# of clip frames', 'clip length',\n",
    "                                     'Grad-CAM time','process and save time', 'total time'])\n",
    "\n",
    "    print('model inference, processing, and saving')\n",
    "\n",
    "    fps = 5\n",
    "\n",
    "    for i in range(len(video_log)):\n",
    "        video_info = video_log.iloc[i]\n",
    "\n",
    "        idx, video_name, frame_save_time, n_frame = (video_info['index'], \n",
    "                                                     video_info['video'], \n",
    "                                                     video_info['elapsed time'], \n",
    "                                                     video_info['number of frames'])\n",
    "\n",
    "        print('')\n",
    "        print(idx, ':', video_name, n_frame, '\\n')\n",
    "\n",
    "        frame_dir = os.path.join(args.frame_root, '{:02d}'.format(idx))\n",
    "\n",
    "        # 프레임을 numpy 형태로 불러옴 & 걸린 시간도 가져옴\n",
    "        frames_pre, frame_load_time = load_frames(frame_dir)\n",
    "\n",
    "        # Binary Model과 이미지 파일을 model_pred_and_time 함수에 넣고 리턴으로 binary model의 예측 결과와 걸린 시간을 가져옴\n",
    "        # 예측결과는 model의 softmax 결과 값이 담긴 list\n",
    "        preds, pred_time = model_pred(model, frames_pre)\n",
    "\n",
    "        # 각각의 예측 모델이 예측한 frame은 softmax의 argmax 결과가 1인 index에 해당하는 frame\n",
    "        # prediction에 해당하는 index만 가져옴 \n",
    "        pred_idx = np.where(np.argmax(preds, axis = 1) == 1)[0]\n",
    "        # prediction index 전후 5프레임씩 더하여, 비디오 클립에 필요한 index를 가져옴\n",
    "        clip_idx = find_clip_idxs(pred_idx, len(frames_pre))\n",
    "\n",
    "        # 모델과, 프레임 그리고 grad-cam이 필요한 프레임의 index를 통해 grad cam image를 구함\n",
    "        cams, cam_time = get_gradcam(model, frames_pre, clip_idx, batch_size = 64, target_layers = ['conv7_2'], target_class = None)\n",
    "\n",
    "\n",
    "        save_dir = os.path.join(args.save_root, '{:02d}'.format(idx))\n",
    "        # 원본 frame과 grad-cam frame를 붙여서 아래 위치에 저장함\n",
    "        processing_time = process_and_save(frame_dir, save_dir, frames_pre, cams, preds, clip_idx)\n",
    "\n",
    "        total_time = total_sum_time([frame_save_time, frame_load_time, pred_time, cam_time, processing_time])\n",
    "\n",
    "        # 파일 저장 과정에서 단계별로  소요된 시간 계산\n",
    "        record.loc[i] = [idx, video_name, n_frame, Sec_to_M_S_ms(n_frame/fps), frame_save_time, frame_load_time, \n",
    "                         pred_time, len(pred_idx), len(clip_idx), Sec_to_M_S_ms(len(clip_idx)/fps),\n",
    "                         cam_time, processing_time, total_time]\n",
    "\n",
    "\n",
    "    record.to_csv(args.save_root + '/record.csv',encoding='utf-8-sig', index = False)\n",
    "    \n",
    "    return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # invoke the main function of the script\n",
    "    main(parse_arguments())\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "cd /home/yunseob\n",
    "source Pytorch/bin/activate\n",
    "\n",
    "cd /mnt/disk1/yunseob/Pytorch/1_CapsuleEndo/algorithms\n",
    "\n",
    "python3 model_inf_for_SBEC_video.py --frame_root '/mnt/disk2/data/private_data/SMhospital/capsule/0 data/video/200917 videos for AI feedback' --save_dir '/mnt/disk1/yunseob/Pytorch/1_CapsuleEndo/video_analysis' --gpu_idx 3\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
