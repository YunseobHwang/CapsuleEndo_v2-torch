{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-01T05:32:20.218786Z",
     "start_time": "2020-09-01T05:32:20.163528Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'CE_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f35184b758be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# sys.path.append('/mnt/disk1/yunseob/Pytorch/1_CapsuleEndo/')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mCE_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msm160_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerateLabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mCE_training\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCNN_Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'CE_utils'"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.append('/mnt/disk1/yunseob/Pytorch/1_CapsuleEndo/')\n",
    "from CE_utils import sm160_dataset, GenerateLabel\n",
    "from CE_training import CNN_Training\n",
    "\n",
    "import numpy as np\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T05:37:30.111288Z",
     "start_time": "2020-07-24T05:37:30.091359Z"
    }
   },
   "outputs": [],
   "source": [
    "def sm160_load(tv_rate = 0.9):\n",
    "    neg_train_aug_path, neg_train_x, neg_valid_x = sm160.Get_train_valid_set('n', 'neg', train_rate = tv_rate)\n",
    "    redspot_train_aug_path, redspot_train_x, redspot_valid_x = sm160.Get_train_valid_set('h', 'redspot', train_rate = tv_rate)\n",
    "    angio_train_aug_path, angio_train_x, angio_valid_x = sm160.Get_train_valid_set('h', 'angio', train_rate = tv_rate)\n",
    "    active_train_aug_path, active_train_x, active_valid_x = sm160.Get_train_valid_set('h', 'active', train_rate = tv_rate)\n",
    "    ero_train_aug_path, ero_train_x, ero_valid_x = sm160.Get_train_valid_set('d', 'ero', train_rate = tv_rate)\n",
    "    ulcer_train_aug_path, ulcer_train_x, ulcer_valid_x = sm160.Get_train_valid_set('d', 'ulcer', train_rate = tv_rate)\n",
    "    str_train_aug_path, str_train_x, str_valid_x = sm160.Get_train_valid_set('d', 'str', train_rate = tv_rate)\n",
    "\n",
    "    # 실제 모델이 학습할 augmented training data의 경로를 클래스 기준에 따라 list로 정리  \n",
    "    h_train_aug_path = np.hstack([redspot_train_aug_path, angio_train_aug_path, active_train_aug_path])\n",
    "    d_train_aug_path = np.hstack([ero_train_aug_path, ulcer_train_aug_path, str_train_aug_path])\n",
    "    a_train_aug_path = np.hstack([h_train_aug_path, d_train_aug_path])\n",
    "    train_paths = [neg_train_aug_path, a_train_aug_path]\n",
    "\n",
    "    # epcch마다 모델을 평가할 (not augmented) training set과 validation set 클래스 기준에 따라 정리\n",
    "    h_train_X = np.vstack([redspot_train_x, angio_train_x, active_train_x])\n",
    "    d_train_X = np.vstack([ero_train_x, ulcer_train_x, str_train_x])\n",
    "    a_train_X = np.vstack([h_train_X, d_train_X])\n",
    "\n",
    "    h_valid_X = np.vstack([redspot_valid_x, angio_valid_x, active_valid_x])\n",
    "    d_valid_X = np.vstack([ero_valid_x, ulcer_valid_x, str_valid_x])\n",
    "    a_valid_X = np.vstack([h_valid_X, d_valid_X])\n",
    "\n",
    "    train_X = [neg_train_x, a_train_X]\n",
    "    valid_X = [neg_valid_x, a_valid_X]\n",
    "    \n",
    "    return train_paths, train_X, valid_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T05:37:30.279148Z",
     "start_time": "2020-07-24T05:37:30.271908Z"
    }
   },
   "outputs": [],
   "source": [
    "phase_a = [[0, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 0], [1, 1, 1]]\n",
    "phase_a_label = ['---', 'f--', '-r-', '--b', 'fr-', 'frb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T05:37:30.863548Z",
     "start_time": "2020-07-24T05:37:30.857002Z"
    }
   },
   "outputs": [],
   "source": [
    "sm160 = sm160_dataset(phase = 'train', data = 'sm_x160_v2', pre_a = phase_a[0], pre_b = False, ext_name = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. custom dataloader (equal batch)\n",
    "\n",
    "- 클래스마다 같은 크기의 배치를 들고와 stack한 후에 training batch를 구성\n",
    "- 병변별로 imbalance 존재\n",
    "- 기존에 cv2로 불러오기 때문에 오래 걸리는 거라는 생각..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:57:38.894664Z",
     "start_time": "2020-07-22T11:56:52.369127Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative - negative | augmented training data: 5619 |(For validation) train x: (5619, 3, 512, 512) valid x: (625, 3, 512, 512)\n",
      "hemorrhagic - red_spot | augmented training data: 500 |(For validation) train x: (500, 3, 512, 512) valid x: (56, 3, 512, 512)\n",
      "hemorrhagic - angioectasia | augmented training data: 16 |(For validation) train x: (16, 3, 512, 512) valid x: (2, 3, 512, 512)\n",
      "hemorrhagic - active_bleeding | augmented training data: 385 |(For validation) train x: (385, 3, 512, 512) valid x: (43, 3, 512, 512)\n",
      "depressed - erosion | augmented training data: 663 |(For validation) train x: (663, 3, 512, 512) valid x: (74, 3, 512, 512)\n",
      "depressed - ulcer | augmented training data: 658 |(For validation) train x: (658, 3, 512, 512) valid x: (74, 3, 512, 512)\n",
      "depressed - stricture | augmented training data: 92 |(For validation) train x: (92, 3, 512, 512) valid x: (11, 3, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "train_paths, train_X, valid_X = sm160_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:57:38.914373Z",
     "start_time": "2020-07-22T11:57:38.897211Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_idxs(dataset, batch_size = 32, shuffle = True):\n",
    "\n",
    "    idxs = np.arange(len(dataset))\n",
    "    total_size = len(idxs)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idxs)\n",
    "    start = 0\n",
    "    b_idxs = []\n",
    "    while True:\n",
    "        if total_size > start + batch_size: \n",
    "            b_idxs.append(list(idxs[start:start+batch_size]))  \n",
    "            start += batch_size\n",
    "        elif total_size <= start + batch_size: \n",
    "            b_idxs.append(list(idxs[start:]))\n",
    "            break \n",
    "    return b_idxs\n",
    "\n",
    "def Reshape4torch(img):\n",
    "    \"\"\"\n",
    "    (sample #, height, width, channel) -> (sample #, channel, height, width)\n",
    "    \"\"\"\n",
    "    img = np.transpose(img, (0, 3, 1, 2))\n",
    "    return img\n",
    "    \n",
    "def GenerateLabel(data, cls):\n",
    "#     label = cls*np.ones([data.shape[0]])\n",
    "    label = cls*np.ones([len(data)])\n",
    "    return label\n",
    "\n",
    "def LoadRandomMinibatch(pathlist, cls, batch_size = 50, image_ch = 'bgr'):\n",
    "    import cv2\n",
    "    idx = np.random.choice(len(pathlist), batch_size)\n",
    "    batch_dir = pathlist[idx]\n",
    "    batch_x = []\n",
    "    for i in batch_dir:\n",
    "        temp = cv2.imread(i)\n",
    "        if image_ch == 'bgr':\n",
    "            pass\n",
    "        elif image_ch == 'rgb':\n",
    "            temp = cv2.cvtColor(temp, cv2.COLOR_BGR2RGB)\n",
    "        elif image_ch == 'hsv':\n",
    "            temp = cv2.cvtColor(temp, cv2.COLOR_BGR2HSV)\n",
    "        temp = temp / 255\n",
    "        temp = (temp - 0.5) / 0.5\n",
    "        batch_x.append(temp)\n",
    "    batch_label = GenerateLabel(batch_x, cls)\n",
    "    return Reshape4torch(np.asarray(batch_x)), batch_label\n",
    "\n",
    "def Shuffle(x1, x2):\n",
    "    \"\"\"\n",
    "    random shuffle of two paired data -> x, y = shuffle(x, y)\n",
    "    but, available of one data -> x = shuffle(x, None)\n",
    "    \"\"\"\n",
    "    idx = np.arange(len(x1))\n",
    "    np.random.shuffle(idx)\n",
    "    if type(x1) == type(x2):\n",
    "        return x1[idx], x2[idx] \n",
    "    else:\n",
    "        return x1[idx]\n",
    "    \n",
    "def Torch_Minibatch_Load(train_paths, batch_size = 100, shuffle = False):\n",
    "    x, y = [], []\n",
    "    for X, cls in zip(train_paths, range(len(train_paths))):\n",
    "#             x_i, y_i = RandomMinibatch(X, Y, batch_size)\n",
    "        x_i, y_i = LoadRandomMinibatch2(X, cls, batch_size)\n",
    "        x.append(x_i), y.append(y_i)\n",
    "    x, y = np.concatenate(x), np.concatenate(y)\n",
    "    if shuffle != False:\n",
    "        x, y = Shuffle(x, y)\n",
    "    x, y = torch.tensor(x, device = device).float(), torch.tensor(y, device = device).long()\n",
    "#         x, y = torch.tensor(x, device = 'cpu').float(), torch.tensor(y, device = 'cpu').long()\n",
    "    return x, y\n",
    "\n",
    "def Validation(X, Y, batch_size = 32):\n",
    "    batch_idxs = Batch_idxs(X, batch_size)\n",
    "    output = []\n",
    "    for batch in batch_idxs:\n",
    "        x = torch.tensor(X[batch, :, :, :], device = device).float() \n",
    "        o = model(x)\n",
    "        output.append(o)\n",
    "    output = torch.cat(output)\n",
    "    loss = criterion(output, Y)\n",
    "\n",
    "    _, pred = torch.max(output, 1)\n",
    "    return loss, pred   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:57:38.926218Z",
     "start_time": "2020-07-22T11:57:38.916780Z"
    }
   },
   "outputs": [],
   "source": [
    "def LoadRandomMinibatch2(pathlist, cls, batch_size = 50, image_ch = 'bgr'):\n",
    "    idx = np.random.choice(len(pathlist), batch_size)\n",
    "    batch_dir = pathlist[idx]\n",
    "    batch_x = []\n",
    "    for i in batch_dir:\n",
    "        temp = Image.open(i)\n",
    "        temp = F.to_tensor(temp)\n",
    "        temp = F.normalize(temp, [0.5, 0.5, 0.5], [0.5, 0.5, 0.5],inplace=False)\n",
    "        batch_x.append(temp)\n",
    "    batch_label = GenerateLabel(batch_x, cls)\n",
    "    return torch.stack(batch_x), batch_label\n",
    "\n",
    "def Torch_Minibatch_Load2(train_paths, batch_size = 100, shuffle = False):\n",
    "    x, y = [], []\n",
    "    for X, cls in zip(train_paths, range(len(train_paths))):\n",
    "        x_i, y_i = LoadRandomMinibatch2(X, cls, batch_size)\n",
    "        \n",
    "        x.append(x_i), y.append(y_i)\n",
    "\n",
    "    x, y = torch.cat(x), torch.tensor(np.hstack(y))\n",
    "    if shuffle != False:\n",
    "        x, y = Shuffle(x, y)\n",
    "    x, y = x.to(device = device), y.to(device = device)\n",
    "#     x, y = torch.tensor(x, device = device).float(), torch.tensor(y, device = device).long()\n",
    "#         x, y = torch.tensor(x, device = 'cpu').float(), torch.tensor(y, device = 'cpu').long()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:57:39.054685Z",
     "start_time": "2020-07-22T11:57:38.929614Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:57:42.372103Z",
     "start_time": "2020-07-22T11:57:39.151854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7933, 3, 512, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = torch.tensor(np.concatenate(train_X))\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T12:03:30.596340Z",
     "start_time": "2020-07-22T12:03:30.208632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([885, 3, 512, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_X = torch.tensor(np.concatenate(valid_X))\n",
    "valid_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:57:39.143954Z",
     "start_time": "2020-07-22T11:57:39.056366Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:57:39.148401Z",
     "start_time": "2020-07-22T11:57:39.145607Z"
    }
   },
   "outputs": [],
   "source": [
    "n_data = [5619, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:32:22.496944Z",
     "start_time": "2020-07-22T11:30:12.519405Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# cv2\n",
    "\n",
    "n_epoch = 1\n",
    "n_batch = 16\n",
    "iter_i = 0\n",
    "epoch_i = 0\n",
    "patient_i = 0\n",
    "\n",
    "while True:\n",
    "    iter_i += 1\n",
    "    train_x, train_y = Torch_Minibatch_Load(train_paths, n_batch, shuffle = True)\n",
    "\n",
    "    if iter_i % (np.max(n_data) // n_batch + 1) == 0:   \n",
    "        epoch_i += 1\n",
    "\n",
    "    if epoch_i == n_epoch:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:34:31.155505Z",
     "start_time": "2020-07-22T11:32:22.499347Z"
    }
   },
   "outputs": [],
   "source": [
    "# cv2\n",
    "# validation까지\n",
    "\n",
    "n_epoch = 1\n",
    "n_batch = 16\n",
    "iter_i = 0\n",
    "epoch_i = 0\n",
    "patient_i = 0\n",
    "\n",
    "while True:\n",
    "    iter_i += 1\n",
    "#             train_x, train_y = Torch_Minibatch_Load(train_Xs, train_Ys, n_batch, shuffle = True)\n",
    "    train_x, train_y = Torch_Minibatch_Load(train_paths, n_batch, shuffle = True)\n",
    "\n",
    "#     model.train(); optimizer.zero_grad()\n",
    "\n",
    "    if iter_i % (np.max(n_data) // n_batch + 1) == 0:   \n",
    "        epoch_i += 1\n",
    "        patient_i += 1\n",
    "\n",
    "#         model.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "#             valid_X = torch.tensor(np.concatenate(valid_X))\n",
    "            v_idxs = batch_idxs(valid_X, 16)\n",
    "\n",
    "            for v_idx in v_idxs:\n",
    "                valid_x = valid_X[v_idx]\n",
    "\n",
    "#             if iter_i == n_iter:\n",
    "    if epoch_i == n_epoch:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T12:05:48.921262Z",
     "start_time": "2020-07-22T12:03:40.759542Z"
    }
   },
   "outputs": [],
   "source": [
    "# cv2\n",
    "# validation까지 (데이터증강 전 훈련데이터 포함)\n",
    "# !! 현재 돌리고 있는 코드와 가장 유사함. \n",
    "# train_Y, valid_Y 안 넣은 거 빼고 \n",
    "\n",
    "n_epoch = 1\n",
    "n_batch = 16\n",
    "iter_i = 0\n",
    "epoch_i = 0\n",
    "patient_i = 0\n",
    "\n",
    "while True:\n",
    "    iter_i += 1\n",
    "#             train_x, train_y = Torch_Minibatch_Load(train_Xs, train_Ys, n_batch, shuffle = True)\n",
    "    train_x, train_y = Torch_Minibatch_Load(train_paths, n_batch, shuffle = True)\n",
    "\n",
    "#     model.train(); optimizer.zero_grad()\n",
    "\n",
    "    if iter_i % (np.max(n_data) // n_batch + 1) == 0:   \n",
    "        epoch_i += 1\n",
    "        patient_i += 1\n",
    "\n",
    "#         model.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "    \n",
    "            t_idxs = batch_idxs(train_X, 16)\n",
    "\n",
    "            for t_idx in t_idxs:\n",
    "                train_x = train_X[t_idx]\n",
    "    \n",
    "            v_idxs = batch_idxs(valid_X, 16)\n",
    "\n",
    "            for v_idx in v_idxs:\n",
    "                valid_x = valid_X[v_idx]\n",
    "\n",
    "#             if iter_i == n_iter:\n",
    "    if epoch_i == n_epoch:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T12:03:02.642173Z",
     "start_time": "2020-07-22T12:03:02.632645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:28:02.412512Z",
     "start_time": "2020-07-22T11:25:48.598096Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform에서 쓰는 함수 끌고와서 기존 cv2함수를 교체해봄\n",
    "\n",
    "n_epoch = 1\n",
    "n_batch = 16\n",
    "iter_i = 0\n",
    "epoch_i = 0\n",
    "patient_i = 0\n",
    "\n",
    "while True:\n",
    "    iter_i += 1\n",
    "    train_x, train_y = Torch_Minibatch_Load2(train_paths, n_batch, shuffle = True)\n",
    "\n",
    "    if iter_i % (np.max(n_data) // n_batch + 1) == 0:   \n",
    "        epoch_i += 1\n",
    "\n",
    "    if epoch_i == n_epoch:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:30:12.517082Z",
     "start_time": "2020-07-22T11:28:02.415570Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform에서 쓰는 함수 끌고와서 기존 cv2함수를 교체해봄\n",
    "# validation까지\n",
    "\n",
    "n_epoch = 1\n",
    "n_batch = 16\n",
    "iter_i = 0\n",
    "epoch_i = 0\n",
    "patient_i = 0\n",
    "\n",
    "while True:\n",
    "    iter_i += 1\n",
    "#             train_x, train_y = Torch_Minibatch_Load(train_Xs, train_Ys, n_batch, shuffle = True)\n",
    "    train_x, train_y = Torch_Minibatch_Load2(train_paths, n_batch, shuffle = True)\n",
    "\n",
    "#     model.train(); optimizer.zero_grad()\n",
    "\n",
    "    if iter_i % (np.max(n_data) // n_batch + 1) == 0:   \n",
    "        epoch_i += 1\n",
    "        patient_i += 1\n",
    "\n",
    "#         model.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "#             valid_X = torch.tensor(np.concatenate(valid_X))\n",
    "            v_idxs = batch_idxs(valid_X, 16)\n",
    "\n",
    "            for v_idx in v_idxs:\n",
    "                valid_x = valid_X[v_idx]\n",
    "\n",
    "#             if iter_i == n_iter:\n",
    "    if epoch_i == n_epoch:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform에서 쓰는 함수 끌고와서 기존 cv2함수를 교체해봄\n",
    "# validation까지\n",
    "\n",
    "n_epoch = 1\n",
    "n_batch = 16\n",
    "iter_i = 0\n",
    "epoch_i = 0\n",
    "patient_i = 0\n",
    "\n",
    "while True:\n",
    "    iter_i += 1\n",
    "#             train_x, train_y = Torch_Minibatch_Load(train_Xs, train_Ys, n_batch, shuffle = True)\n",
    "    train_x, train_y = Torch_Minibatch_Load2(train_paths, n_batch, shuffle = True)\n",
    "\n",
    "#     model.train(); optimizer.zero_grad()\n",
    "\n",
    "    if iter_i % (np.max(n_data) // n_batch + 1) == 0:   \n",
    "        epoch_i += 1\n",
    "        patient_i += 1\n",
    "\n",
    "#         model.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "#             valid_X = torch.tensor(np.concatenate(valid_X))\n",
    "            v_idxs = batch_idxs(valid_X, 16)\n",
    "\n",
    "            for v_idx in v_idxs:\n",
    "                valid_x = valid_X[v_idx]\n",
    "\n",
    "#             if iter_i == n_iter:\n",
    "    if epoch_i == n_epoch:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. torch dataset 구성 \n",
    "\n",
    "- class별로 normal과 abnormal을 거의 equal하게 맞추는데,\n",
    "- 대신에 negative size와 지정한 병변의 갯수(6)에 따라 equal하게 negative의 1/N oversampling함 (strict)\n",
    "- oversampling이기 때문에 위에서 랜덤으로 equal하게 뽑는 것보단 편향이 발생하기 더 쉬움\n",
    "- batch loader 시간은 빠른 편"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T05:37:37.401395Z",
     "start_time": "2020-07-24T05:37:37.396020Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T05:37:46.860097Z",
     "start_time": "2020-07-24T05:37:37.660057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative: (6244,)\n",
      "red spot: (556,)\n",
      "angioectasia: (18,)\n",
      "active_bleeding: (428,)\n",
      "erosion: (737,)\n",
      "ulcer: (732,)\n",
      "stricture: (103,)\n"
     ]
    }
   ],
   "source": [
    "neg_path = sm160.load_path('n', 'neg')\n",
    "redspot_path = sm160.load_path('h', 'redspot')\n",
    "angio_path = sm160.load_path('h', 'angio')\n",
    "active_path = sm160.load_path('h', 'active')\n",
    "ero_path = sm160.load_path('d', 'ero')\n",
    "ulcer_path = sm160.load_path('d', 'ulcer')\n",
    "str_path = sm160.load_path('d', 'str')\n",
    "\n",
    "print(\"negative:\",neg_path.shape)\n",
    "print(\"red spot:\",redspot_path.shape)\n",
    "print(\"angioectasia:\",angio_path.shape)\n",
    "print(\"active_bleeding:\",active_path.shape)\n",
    "print(\"erosion:\",ero_path.shape)\n",
    "print(\"ulcer:\",ulcer_path.shape)\n",
    "print(\"stricture:\",str_path.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T05:20:48.425323Z",
     "start_time": "2020-07-24T05:20:48.043069Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T05:50:19.447824Z",
     "start_time": "2020-07-24T05:50:19.407123Z"
    }
   },
   "outputs": [],
   "source": [
    "class CE_dataset(Dataset):\n",
    "    def __init__(self, path, data_dir = 'sm_x160_v2',\n",
    "                 class_config = {'n': ['neg'], \n",
    "                                 'a': {'h': ['redspot', 'angio', 'active'], \n",
    "                                       'd': ['ero', 'ulcer', 'str']}},\n",
    "                 oversample=False, mode = 'binary', transform=None,\n",
    "                 valid_separate=False, valid_df=None):\n",
    "    \n",
    "        self.class_config = class_config\n",
    "        self.oversample = oversample\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.data_dir = os.path.join(path[0].split(data_dir)[0], data_dir)\n",
    "        \n",
    "        self.lesions = dict(neg = 'negative', \n",
    "                       redspot = 'red_spot', angio = 'angioectasia', active = 'active_bleeding', \n",
    "                       ero = 'erosion', ulcer = 'ulcer', str = 'stricture', \n",
    "                       amp = 'ampulla_of_vater', lym = 'lymphoid_follicles', tum = 'small_bowel_tumor')\n",
    "        self.types = dict(n = 'negative', h = 'hemorrhagic', d = 'depressed', p = 'protruded')\n",
    "        \n",
    "        self.types_cols = ([self.types['n']]*len(self.class_config['n']) + \n",
    "                           list(np.concatenate([[self.types[i]]*\n",
    "                                                len(self.class_config['a'][i]) \n",
    "                                                for i in self.class_config['a'].keys()])))\n",
    "        self.lesion_cols = ([self.lesions[i] for i in self.class_config['n']] + \n",
    "                       [self.lesions[i] for i in list(np.concatenate(\n",
    "                           [i for i in self.class_config['a'].values()]))])\n",
    "#         self.lesion_cols = ['negative', \n",
    "#                        'red_spot', 'angioectasia', 'active_bleeding', \n",
    "#                        'erosion', 'ulcer', 'stricture',\n",
    "#                        'ampulla_of_vater', 'lymphoid_follicles', 'small_bowel_tumor']\n",
    "\n",
    "        if valid_separate:\n",
    "            self.subpath = [subpath.split(data_dir)[-1][1:] for subpath in path]\n",
    "            \n",
    "            self.df = pd.DataFrame(columns = [self.types_cols,\n",
    "                                              self.lesion_cols])\n",
    "            self.df_config()\n",
    "            self.df, self.valid_df = self.df_train_valid_split()\n",
    "            \n",
    "        elif not valid_separate and valid_df is not None:\n",
    "            self.df = valid_df\n",
    "            \n",
    "        if self.oversample:\n",
    "            self.df_os = self.oversampling()\n",
    "\n",
    "    def df_config(self): \n",
    "        names = []\n",
    "        splits = []\n",
    "        for i, path in enumerate(self.subpath):\n",
    "            split, category1, category2, name = path.split('/')\n",
    "            self.df.loc[i, (category1, category2)] = 1\n",
    "            names.append(name), splits.append(split) \n",
    "\n",
    "        self.df.insert(0, 'name', names)\n",
    "        self.df['split'] = splits\n",
    "        self.df.fillna(0, inplace = True)\n",
    "        \n",
    "    def df_train_valid_split(self):\n",
    "        train_df = []\n",
    "        valid_df = []\n",
    "\n",
    "        for i in self.class_config.keys():\n",
    "            for j in self.class_config[i]:\n",
    "                if i == 'a':\n",
    "                    for k in self.class_config[i][j]:\n",
    "                        a_df = self.df[self.df[self.types[j], self.lesions[k]] == 1] \n",
    "                        valid_idx = np.random.choice(len(a_df), int(0.2*len(a_df)), replace = False)\n",
    "                        train_idx = np.setdiff1d(np.arange(len(a_df)), valid_idx)\n",
    "                        valid_df.append(a_df.iloc[valid_idx])\n",
    "                        train_df.append(a_df.iloc[train_idx])\n",
    "                else:\n",
    "                    n_df = self.df[self.df[self.types[i], self.lesions[j]] == 1] \n",
    "                    valid_idx = np.random.choice(len(n_df), int(0.2*len(n_df)), replace = False)\n",
    "                    train_idx = np.setdiff1d(np.arange(len(n_df)), valid_idx)\n",
    "                    valid_df.append(n_df.iloc[valid_idx])\n",
    "                    train_df.append(n_df.iloc[train_idx])\n",
    "\n",
    "        train_df = pd.concat(train_df)\n",
    "        valid_df = pd.concat(valid_df)\n",
    "        \n",
    "        return train_df, valid_df\n",
    "\n",
    "    def oversampling(self):\n",
    "        n_df = self.df[self.df[self.types['n'], self.lesions[self.class_config['n'][0]]] == 1]\n",
    "        n_max = len(n_df)\n",
    "\n",
    "        n_a = 0\n",
    "        for les in self.class_config['a'].values():\n",
    "            n_a += len(les)\n",
    "\n",
    "        n_ref = int(np.max(n_max) / (n_a))\n",
    "\n",
    "        new_a_dfs = []\n",
    "        for j in self.class_config['a']:\n",
    "            for k in self.class_config['a'][j]:\n",
    "                cls, les = self.types[j], self.lesions[k]\n",
    "                a_df = self.df[self.df[cls, les] == 1]\n",
    "\n",
    "                quo = n_ref//len(a_df)\n",
    "                rem = n_ref%(len(a_df))\n",
    "                rest_idx  = np.random.choice(len(a_df), rem, replace = False)\n",
    "\n",
    "                a_df_os = pd.concat([a_df]*quo)\n",
    "                a_df_os = a_df_os.append(a_df.iloc[rest_idx])\n",
    "                new_a_dfs.append(a_df_os)\n",
    "        new_a_df = pd.concat(new_a_dfs)\n",
    "        new_df = new_a_df.append(n_df)\n",
    "\n",
    "        return new_df\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         if self.oversample:\n",
    "#             self.df_os = self.oversampling()\n",
    "        if self.oversample:\n",
    "            row = self.df_os.iloc[index]\n",
    "        else:\n",
    "            row = self.df.iloc[index]\n",
    "        sp, n = row['split'].tolist()[0], row['name'].tolist()[0]\n",
    "        c1, c2 = row[row == 1].index.values[0]\n",
    "\n",
    "        X = Image.open(os.path.join(self.data_dir, sp, c1, c2, n))\n",
    "\n",
    "        target = []\n",
    "\n",
    "        if self.mode == 'binary':\n",
    "            if row['negative']['negative'] == 1:\n",
    "                target.append(0)\n",
    "            else:\n",
    "                target.append(1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "\n",
    "        if target:\n",
    "            target = tuple(target) if len(target) > 1 else target[0]\n",
    "\n",
    "#             if self.target_transform is not None:\n",
    "#                 target = self.target_transform(target)\n",
    "        else:\n",
    "            target = None\n",
    "\n",
    "        return X, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.oversample:\n",
    "            self.df_os = self.oversampling()\n",
    "            return len(self.df_os)\n",
    "        elif not self.oversample:\n",
    "            return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T05:38:13.761677Z",
     "start_time": "2020-07-24T05:37:57.028236Z"
    }
   },
   "outputs": [],
   "source": [
    "train_path = list(np.hstack([neg_path, \n",
    "                              redspot_path, angio_path, active_path, \n",
    "                              ero_path, ulcer_path, str_path]))\n",
    "\n",
    "transformer = transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                 ])\n",
    "\n",
    "class_config = {'n': ['neg'], \n",
    "                'a': {'h': ['redspot', 'angio', 'active'], \n",
    "                      'd': ['ero', 'ulcer', 'str']}}\n",
    "\n",
    "training_dataset = CE_dataset(train_path, data_dir = 'sm_x160_v2',\n",
    "                             class_config = class_config,\n",
    "                              oversample=True, mode = 'binary', transform=transformer,\n",
    "                              valid_separate=True)\n",
    "\n",
    "validation_dataset = CE_dataset(train_path, data_dir = 'sm_x160_v2',\n",
    "                             class_config = class_config,\n",
    "                              oversample=False, mode = 'binary', transform=transformer,\n",
    "                              valid_separate=False, valid_df=training_dataset.valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:34:59.966941Z",
     "start_time": "2020-07-22T11:34:59.877145Z"
    }
   },
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(dataset=training_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:35:00.405870Z",
     "start_time": "2020-07-22T11:34:59.968752Z"
    }
   },
   "outputs": [],
   "source": [
    "x, y = training_loader.__iter__().__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:35:00.414243Z",
     "start_time": "2020-07-22T11:35:00.408648Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 512, 512])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:35:00.422321Z",
     "start_time": "2020-07-22T11:35:00.416627Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:35:00.476038Z",
     "start_time": "2020-07-22T11:35:00.424536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9988"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:35:00.525366Z",
     "start_time": "2020-07-22T11:35:00.477734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:37:00.134098Z",
     "start_time": "2020-07-22T11:35:00.528619Z"
    }
   },
   "outputs": [],
   "source": [
    "# oversampling 위로 뺐을때 ..\n",
    "\n",
    "for i, (inputs, outputs) in enumerate(training_loader):\n",
    "    x, y = inputs.to(device), outputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T11:39:20.914708Z",
     "start_time": "2020-07-22T11:37:00.136170Z"
    }
   },
   "outputs": [],
   "source": [
    "# oversampling 위로 뺐을때 ..\n",
    "# vallidation 까지\n",
    "\n",
    "for i, (inputs, outputs) in enumerate(training_loader):\n",
    "    x, y = inputs.to(device), outputs.to(device)\n",
    "\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, outputs) in enumerate(validation_loader):\n",
    "            x, y = inputs.to(device), outputs.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Conclusion\n",
    "\n",
    "- 의외로 속도면에서 차이가 별로 안 나..? (cv2로 짜놓은 것도 그렇게 구리진 않았다)\n",
    "- 지금 속도가 걸리는 건 validation을 데이터증강전의 train_X까지 해서 ...? (아니네)\n",
    "- Lr 3개 Bs 3개 9개 조합 뒤로 갈수록 데이터 많아지니 오래 걸리는 듯 (이게며칠째야)\n",
    "- 병변별로도 strict하게 나누는 것이 과연 모델 성능에 영향이 있을 것인지 의문"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
